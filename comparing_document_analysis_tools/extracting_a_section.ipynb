{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting a section from a DOCX file using LLMS and RAG\n",
    "\n",
    "If your `DOCX` file is properly formatted, you do not need to do every step in this notebook. Current document analyzers such as `unstructured` or `docling` do a great job on that. However, there are cases where the `DOCX` files is not formatted using headings or titles, and in those cases, these tools will not solve the issue of extracting a specific section of the document.\n",
    "\n",
    "In the case above, extracting the sections of the document with the code below would be a good solution.\n",
    "\n",
    "```python\n",
    "from docx import Document\n",
    "\n",
    "def extract_sections(docx_file, keywords):\n",
    "    doc = Document(docx_file)\n",
    "    extracted_sections = {}\n",
    "    current_heading = None\n",
    "\n",
    "    for paragraph in doc.paragraphs:\n",
    "        if paragraph.style.name.startswith('Heading'):\n",
    "            current_heading = paragraph.text\n",
    "        elif current_heading and any(keyword in paragraph.text.lower() for keyword in keywords):\n",
    "            extracted_sections[current_heading] = extracted_sections.get(current_heading, '') + '\\n' + paragraph.text\n",
    "\n",
    "    return extracted_sections\n",
    "\n",
    "```\n",
    "\n",
    "However, we are focused on files where there is no section beginning with the formatting `paragraph.style.name.startswith('Heading')`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and basic setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.word_document import (\n",
    "    UnstructuredWordDocumentLoader,\n",
    "    Docx2txtLoader,\n",
    "    )\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "import os\n",
    "\n",
    "# If you want to hide your API keys in an .env file, do something like:\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Example: set your OpenAI API key as an environment variable\n",
    "#os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n",
    "# Now retrieve the token from environment variables\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Set the environment variable that huggingface_hub / transformers will look for\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = hf_token\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "DOCX_FILES = glob('./data/docx/*')\n",
    "\n",
    "\n",
    "# Organize the list of files making sure that the same file names are in the same index\n",
    "DOCX_FILES.sort()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and Parsing the DOCX\n",
    "LangChain’s default `UnstructuredFileLoader` will try to parse the file using Unstructured under the hood.\n",
    "\n",
    "As seen in the another notebook, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_document_text(file, engine='docx2txt'):\n",
    "    \"\"\"\n",
    "    Extracts the text from a document file.\n",
    "    \"\"\"\n",
    "    if engine == 'docx2txt':\n",
    "        loader = Docx2txtLoader(file)\n",
    "        raw_documents = loader.load()\n",
    "    elif engine == 'unstructured':\n",
    "        loader = UnstructuredWordDocumentLoader(file)\n",
    "        raw_documents = loader.load()\n",
    "    else:\n",
    "        raise ValueError('Invalid engine')\n",
    "    return raw_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 1\n",
      "Latrophilin-2 mediates fluid shear stress mechanotransduction at endothelial junctions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Keiichiro Tanaka1,7,*, Minghao Chen1,7, Andrew Prendergast1, Zhenwu Zhuang1, Ali Nasiri2, Divyesh Joshi1, Jared Hintzen1, Minhwan Chung1, Abhishek Kumar1, Arya Mani1, Anthony Koleske3, Jason Crawford4, Stefania Nicoli1 and Martin A. Schwartz1,5,6,*\n",
      "\n",
      "\n",
      "\n",
      "1 Yale Cardiovascular Research Center, Section of Cardiovascular Medicine, Department of Internal Medicine, School of Medicine, Yale University, New Haven\n"
     ]
    }
   ],
   "source": [
    "# raw_documents is a list of LangChain Document objects\n",
    "raw_documents = extract_document_text(DOCX_FILES[0], engine='docx2txt')\n",
    "print(f\"Number of documents: {len(raw_documents)}\")\n",
    "print(raw_documents[0].page_content[:500])  # First 500 characters of the first doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 1\n",
      "Latrophilin-2 mediates fluid shear stress mechanotransduction at endothelial junctions\n",
      "\n",
      "Keiichiro Tanaka1,7,*, Minghao Chen1,7, Andrew Prendergast1, Zhenwu Zhuang1, Ali Nasiri2, Divyesh Joshi1, Jared Hintzen1, Minhwan Chung1, Abhishek Kumar1, Arya Mani1, Anthony Koleske3, Jason Crawford4, Stefania Nicoli1 and Martin A. Schwartz1,5,6,*\n",
      "\n",
      "1 Yale Cardiovascular Research Center, Section of Cardiovascular Medicine, Department of Internal Medicine, School of Medicine, Yale University, New Haven, CT 065\n"
     ]
    }
   ],
   "source": [
    "# raw_documents is a list of LangChain Document objects\n",
    "raw_documents = extract_document_text(DOCX_FILES[0], engine='unstructured')\n",
    "print(f\"Number of documents: {len(raw_documents)}\")\n",
    "print(raw_documents[0].page_content[:500])  # First 500 characters of the first doc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see a very important difference here, which is the time used by each cell to run. As showed in the `comparing_performance-extraction` notebook, the `docx2text` engine shows an order of magnitude faster extraction than the `unstructured` engine. For this reason, we will keep using `docx2text` in this notebook.\n",
    "\n",
    "# Splitting into Chunks\n",
    "We want to break the text into smaller chunks (so that each chunk is not too large for embeddings or retrieval). We can use `RecursiveCharacterTextSplitter`.\n",
    "\n",
    "Here we are using ```RecursiveCharacterTextSplitter``` to split the text into chunks of 1024 characters. However, using a splitter based on `NLTKTextSplitter` or `SpacyTextSplitter` might be a good option to keep the chunks semantically meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks after splitting: 137\n",
      "kinases (SFKs) within seconds, resulting in ligand-independent activation of VEGF receptors and downstream events including endothelial cell (EC) alignment in the direction of flow(Collins et al, 2012). However, the mechanisms by which proteins located at cell-cell junctions can sense forces from shear stress exerted on the apical surface is unclear. Previous work suggests that activation of the junctional complex is not primary but rather requires an upstream event, mediated presumably by another mechanosensor (Conway et al, 2013).\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=768,   # characters per chunk\n",
    "    chunk_overlap=64  # overlap for context\n",
    ")\n",
    "\n",
    "text_splitted = text_splitter.split_documents(raw_documents)\n",
    "\n",
    "print(f\"Number of chunks after splitting: {len(text_splitted)}\")\n",
    "print(text_splitted[6].page_content)  # A sample chunk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding the section into a vectorstore for RAG\n",
    "\n",
    "We use here `HuggingFace`'s `sentence-transformers` to embed the text chunks into vectors. We then store these vectors in a `VectorStore` for RAG to use.\n",
    "\n",
    "One question I always had regarding the embedding tokenizer and the RAG was whether we needed to prepare the embeddings using the same model or tokenizer used for the LLM inference. I asked this and obtained the following answer from ChatGPT:\n",
    "\n",
    "**Embeddings vs. LLM Inference: They Don’t Have to Match**\n",
    "\n",
    "In a typical “Retrieval-Augmented Generation” (RAG) flow:\n",
    "\n",
    "* **Embeddings**: You take text chunks and convert them into embedding vectors (using, e.g., sentence-transformers/all-MiniLM-L6-v2).\n",
    "* **Vector Store**: You store those embeddings in a vector database (e.g., Chroma).\n",
    "* **Retrieval**: When a query arrives (e.g., “Give me the Data Availability statement”), you embed that query with the same embedding model and find the most similar chunks.\n",
    "* **Generation**: You feed those retrieved chunks (as plain text) into your Large Language Model (LLM) to answer the query or provide the final text.\n",
    "Because the LLM only sees or generates plain text, there’s no requirement that the LLM use the same tokenizer or embedding model you used for retrieval. They are effectively separate steps:\n",
    "\n",
    "* Embedding Model → Produces vector embeddings\n",
    "* LLM → Takes text, outputs text\n",
    "\n",
    "Hence, you can absolutely use sentence-transformers/all-MiniLM-L6-v2 for embedding, and any local or remote LLM for the generation step—OpenAI’s GPT-4, Anthropic’s Claude, or a local Hugging Face model like Falcon, Mistral, LLaMA-2, etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Initialize the embedding model\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "# 2) Create (or load) a Chroma vector store\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "persist_directory = \"./chroma_db\"  # Where to store DB files on disk\n",
    "\n",
    "# Build the vector store from our docs\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=text_splitted,\n",
    "    embedding=embedding_model,\n",
    "    persist_directory=persist_directory\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the LLM pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms.base import LLM\n",
    "from typing import Optional, List, Any\n",
    "import re\n",
    "\n",
    "from langchain.llms.base import LLM\n",
    "from pydantic import Field, BaseModel\n",
    "from typing import Optional, List, Any\n",
    "\n",
    "from langchain.llms.base import LLM\n",
    "from pydantic import Field\n",
    "from typing import Optional, List, Any\n",
    "\n",
    "class CTransformersLLM(LLM):\n",
    "    \"\"\"\n",
    "    A custom LangChain LLM wrapper around a ctransformers-based .gguf model.\n",
    "    \"\"\"\n",
    "\n",
    "    ctransformers_model: Any = Field(default=None)\n",
    "    temperature: float = Field(default=0.0)\n",
    "    max_tokens: int = Field(default=256)\n",
    "\n",
    "    class Config:\n",
    "        # Let Pydantic store arbitrary Python objects (your ctransformers model).\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "    def __init__(self, ctransformers_model: Any, **kwargs):\n",
    "        \"\"\" \n",
    "        ctransformers_model: The loaded model from ctransformers.AutoModelForCausalLM. \n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        # Bypass pydantic's immutability checks:\n",
    "        object.__setattr__(self, \"ctransformers_model\", ctransformers_model)\n",
    "        object.__setattr__(self, \"temperature\", self.temperature)\n",
    "        object.__setattr__(self, \"max_tokens\",  self.max_tokens)\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        \"\"\"A short string identifying this LLM type.\"\"\"\n",
    "        return \"ctransformers\"\n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None, **kwargs: Any) -> str:\n",
    "        \"\"\"\n",
    "        The main generation method that LangChain calls internally.\n",
    "        `prompt` is the text prompt.\n",
    "        `stop` is an optional list of stop tokens.\n",
    "        \"\"\"\n",
    "        output = self.ctransformers_model(\n",
    "            prompt,\n",
    "            max_new_tokens=self.max_tokens,\n",
    "            temperature=self.temperature,\n",
    "        )\n",
    "        if stop:\n",
    "            for token in stop:\n",
    "                if token in output:\n",
    "                    output = output.split(token)[0]\n",
    "        return output\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> dict:\n",
    "        \"\"\"\n",
    "        Return a dictionary describing your model. \n",
    "        LangChain calls `dict(self._identifying_params)` internally.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"model_type\": \"ctransformers\",\n",
    "            \"temperature\": self.temperature,\n",
    "            \"max_tokens\": self.max_tokens\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 1 files: 100%|██████████| 1/1 [00:00<00:00, 23696.63it/s]\n",
      "Fetching 1 files: 100%|██████████| 1/1 [00:00<00:00, 23172.95it/s]\n"
     ]
    }
   ],
   "source": [
    "from ctransformers import AutoModelForCausalLM\n",
    "\n",
    "# Replace with any GGUF model path/repo you want\n",
    "model_id = \"TheBloke/Mistral-7B-v0.1-GGUF\"\n",
    "model_file = \"mistral-7b-v0.1.Q4_0.gguf\"  # Example quant file name\n",
    "model_type = \"mistral\"  # ctransformers needs to know the base model type\n",
    "# model_id = \"QuantFactory/Meta-Llama-3-8B-GGUF\"\n",
    "# model_file = \"Meta-Llama-3-8B.Q4_0.gguf\"\n",
    "# model_type = \"llama\"  # ctransformers needs to know the base model type\n",
    "# This downloads the .gguf file from HF (if not already cached)\n",
    "llm = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    model_file=model_file,\n",
    "    model_type=model_type,  # ctransformers needs to know the base model type\n",
    "    # or \"llama\" for Llama 2, etc.\n",
    "    # Below are optional arguments:\n",
    "    # gpu_layers=100% if you want MPS acceleration on macOS with Apple Silicon\n",
    "    # or `gpu_layers=50` etc. The more layers you put on the GPU, the more VRAM is used.\n",
    "    # For MPS on macOS, set:\n",
    "    # force_download=True,  # If you want to force re-download\n",
    "    gpu_layers=50,\n",
    "    local_files_only=False # If True, won't download but only use local\n",
    ")\n",
    "\n",
    "# prompt = \"Explain why 4-bit quantization is beneficial.\"\n",
    "# output = llm(prompt, max_new_tokens=200)\n",
    "# print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose `llm` is your ctransformers model\n",
    "ctrans_llm = CTransformersLLM(\n",
    "    ctransformers_model=llm,\n",
    "    temperature=0.1,\n",
    "    max_tokens=128\n",
    "    )\n",
    "\n",
    "# output = ctrans_llm(\"Explain the meaning of life in 20 words.\")\n",
    "# print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the retrieval QA chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import UnstructuredFileLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})\n",
    "\n",
    "qa_chain_local = RetrievalQA.from_chain_type(\n",
    "    llm=ctrans_llm,         # Our ctransformers-based LLM\n",
    "    chain_type=\"refine\",    # \"stuff\", \"refine\", or \"map_reduce\"\n",
    "    retriever=retriever,\n",
    ")\n",
    "\n",
    "# Example Queries\n",
    "query = \"Please provide the Data Availability statement from this document. Extract the estatemen exactly as it is written in the document. Make sure not to repeat the estatement twice.\"\n",
    "response = qa_chain_local.invoke(query)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Availability estatement:\n",
      " \n",
      "Data availability\n",
      "\n",
      "The datasets produced in this study are available in the following databases:\n",
      "\n",
      "All data: BioStudies S-BIAD928: https://www.ebi.ac.uk/biostudies/studies?query=S-BIAD928\n",
      "\n",
      "------------\n",
      "Context information\n",
      "\n",
      "The datasets produced in this study are available in the following databases:\n",
      "\n",
      "All data: BioStudies S-BIAD928: https://www.ebi.ac.uk/biostudies/studies?query=S-BIAD92\n"
     ]
    }
   ],
   "source": [
    "print(\"Data Availability estatement:\\n\", response[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We check now how does the OpenAI performance with this method work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 10})\n",
    "\n",
    "openai_llm = OpenAI(\n",
    "    model=\"gpt-3.5-turbo-instruct\",\n",
    "    temperature=0,\n",
    "    max_retries=2,\n",
    "    max_tokens=512,\n",
    "    frequency_penalty=2\n",
    ")\n",
    "\n",
    "qa_chain_openai = RetrievalQA.from_chain_type(\n",
    "    llm=openai_llm,         # Our ctransformers-based LLM\n",
    "    chain_type=\"refine\",    # \"stuff\", \"refine\", or \"map_reduce\"\n",
    "    retriever=retriever,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Queries\n",
    "query = \"Please provide the Data Availability statement from this document. Extract the estatemen exactly as it is written in the document. Make sure not to repeat the estatement twice.\"\n",
    "response = qa_chain_openai.invoke(query)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The datasets produced in this study are available in the following databases:\n",
      "\n",
      "All data: BioStudies S-BIAD928: https://www.ebi.ac.uk/biostudies/studies?query=S-BIAD928\n"
     ]
    }
   ],
   "source": [
    "print(response[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ground truth**\n",
    "\n",
    "```\n",
    "Data availability\n",
    "The datasets produced in this study are available in the following databases:\n",
    "All data: BioStudies S-BIAD928: https://www.ebi.ac.uk/biostudies/studies?query=S-BIAD928\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Queries\n",
    "query = \"\"\"\n",
    "    At the end of the document you will find the section of the figure legends or figure captions. \n",
    "    I want you to extrac literally as it is shown in the text the caption of Figure 1.\n",
    "    \n",
    "    Extract the text as it appears in the document.\n",
    "\"\"\"\n",
    "response = qa_chain_openai.invoke(query)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Figure 1. Confocal imaging of intersegmental vessels and dorsal aorta in zebrafish larvae. Larvae were fixed, permeabilized, and stained with primary antibody (1:200 rabbit anti-GFP A-11122, ThermoFisher) overnight at 4°C followed by incubation with secondary antibody (Alexa Fluor® 488 goat anti-rabbit IgG H+L A11034). The images show Gα proteins specific for endothelial flow responses in the vasculature of zebrafish larvae. This figure demonstrates the localization of these proteins within the vasculature using confocal microscopy techniques.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(response[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ground truth**\n",
    "\n",
    "```\n",
    "    Figure 1. Gα proteins specific for endothelial flow responses\n",
    "    A, Flow-induced endothelial alignment after Gα knockdown.  HUVECs were subjected to fluid shear stress (FSS) at a rate of 12 dynes/cm2 for 16 hours and nuclear orientation quantified as histograms showing the percentage of cells within each 10° of the flow directions from 0° to 90° (see Methods) ****: p<0.0001; one-way ANOVA with Tukey’s multiple comparisons test. B, Src family kinase activation, quantified in C. n=5 for control, Gi knockdown, Gq11 knockdown and n=3 for simultaneous knockdown of Gi and Gq11.  Values are means ± SEM. ****: p<0.0001, ***: p<0.001; one-way ANOVA with Tukey multiple comparison test.  D, Rescue of Gq/11 and Gi knockdown by re-expression of siRNA-resistant versions of the indicated proteins. ****: p<0.0001; one-way ANOVA with Tukey’s multiple comparison test. E, Amino acid sequences of Gαi1, Gαi2, and Gαi3 at the mutation sites of Gαi1 gain-of-function mutant.  F, Rescue of Gq/11 and Gi knockdown with indicated Gα proteins. Each point corresponds to one measurement averaged from >500 cells. N=4. ****: p<0.0001; one-way ANOVA with Tukey’s multiple comparisons test.  G, GINIP pulldown assay for activation of Gαi2 by FSS.  N=3. Results quantified in H.  **: p=0.0185, Student’s t-test.  I, GRK2N pulldown assay for activation of Gq.  N=4, quantified in J. *: p<0.05, Student’s t-test. K, GINIP pulldown assay for FSS-induced activation of wild type and Q306K Gαi1.  N = 4, quantified in L. *: p=0.0304, ***: p=0.0017; Student’s t-test. M, Gi2 activation after PECAM-1 knockdown. HUVECs expressing GluGlu tagged Gi2 were transfected with scrambled siRNA or PECAM-1 siRNA, exposed to FSS, and Gi2 activation assayed as described above, quantified in N. Values are means ± SEM, normalized to input Gα protein levels. ***: p<0.001; Student’s t-test, N=4.\n",
    "```\n",
    "\n",
    "\n",
    "Looks like the extraction of the `data availability` section, being short enough and likely not to be repeated for the rest of the paper, was successful. However, the extraction of the `Figure 1` section was not successful. This is likely due to the fact that the section is too long and the embeddings are not able to capture the entire section in a single chunk. Increasing the chunk size does not seem to help either, so we will stop pursuing the figure legend extraction in this way.\n",
    "\n",
    "## Benchmarking `Data Availability` extraction\n",
    "\n",
    "Now, having an open model and an Open AI model doing a similar task, we will average their extraction quality over the entire set of documents we have to see how reliable this method of extraction might be on the long term:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character '°' (U+00B0) (3847400222.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[87], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    A, Flow-induced endothelial alignment after Gα knockdown.  HUVECs were subjected to fluid shear stress (FSS) at a rate of 12 dynes/cm2 for 16 hours and nuclear orientation quantified as histograms showing the percentage of cells within each 10° of the flow directions from 0° to 90° (see Methods) ****: p<0.0001; one-way ANOVA with Tukey’s multiple comparisons test. B, Src family kinase activation, quantified in C. n=5 for control, Gi knockdown, Gq11 knockdown and n=3 for simultaneous knockdown of Gi and Gq11.  Values are means ± SEM. ****: p<0.0001, ***: p<0.001; one-way ANOVA with Tukey multiple comparison test.  D, Rescue of Gq/11 and Gi knockdown by re-expression of siRNA-resistant versions of the indicated proteins. ****: p<0.0001; one-way ANOVA with Tukey’s multiple comparison test. E, Amino acid sequences of Gαi1, Gαi2, and Gαi3 at the mutation sites of Gαi1 gain-of-function mutant.  F, Rescue of Gq/11 and Gi knockdown with indicated Gα proteins. Each point corresponds to one measurement averaged from >500 cells. N=4. ****: p<0.0001; one-way ANOVA with Tukey’s multiple comparisons test.  G, GINIP pulldown assay for activation of Gαi2 by FSS.  N=3. Results quantified in H.  **: p=0.0185, Student’s t-test.  I, GRK2N pulldown assay for activation of Gq.  N=4, quantified in J. *: p<0.05, Student’s t-test. K, GINIP pulldown assay for FSS-induced activation of wild type and Q306K Gαi1.  N = 4, quantified in L. *: p=0.0304, ***: p=0.0017; Student’s t-test. M, Gi2 activation after PECAM-1 knockdown. HUVECs expressing GluGlu tagged Gi2 were transfected with scrambled siRNA or PECAM-1 siRNA, exposed to FSS, and Gi2 activation assayed as described above, quantified in N. Values are means ± SEM, normalized to input Gα protein levels. ***: p<0.001; Student’s t-test, N=4.\u001b[0m\n\u001b[0m                                                                                                                                                                                                                                                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character '°' (U+00B0)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
